\chapter{Algorithms for the Online Portfolio Optimization Problem}\label{ch:algos}

In this section we will review the state of the art algorithms for the Online Portfolio Optimization problem and discuss their theoretical guarantees, and how this algorithms can be generated by the theoretical framework of Online Learning with expert advice and Online Optimization we described in Chapter \ref{ch:OnlineLearning}.

The setting is the one described in Section \ref{sec:OPO}, in particular $\Delta=\Delta_{N-1}\subset \mathbb R^N$ is the $N$-simplex, and an element $\mathbf x_t\in\Delta$ describes the allocation into $N$ stocks for the $t$-th period.

As is commonly done in the portfolio allocation literature~\cite{agarwal2006algorithms}, we assume that the price of the assets does not change too much during two consecutive rounds, or, formally:

\begin{assumption} \label{ass:nojunk}
    %  Given the OPO framework, 
     There exist two finite constants $\epsilon_l, \epsilon_u \in \mathbb{R}^+$ s.t.~the price relatives~$y_{j,t} \in [\epsilon_l, \epsilon_u]$, with $0 < \epsilon_l \leq \epsilon_u < +\infty$, for each round $t \in \{ 1, \ldots, T \}$ and each asset $j \in \{1, \ldots, N \}$.
\end{assumption}

Notice that under Assumption~\ref{ass:nojunk}, it is possible to bound the $L_1$, $L_2$ and the $L_\infty$ gradient of the loss as follows:\todo{check}

\begin{equation} \label{eq:bounded_gradient}
    ||\nabla \log (\langle \mathbf{x}_t, \mathbf{y}_t) \rangle||_1 \leq \frac{N\epsilon_u}{\epsilon_l}:=G_1.
\end{equation}

\begin{equation} \label{eq:bounded_gradient}
    ||\nabla \log (\langle \mathbf{x}_t, \mathbf{y}_t) \rangle||_2 \leq \frac{\epsilon_u \sqrt{N}}{\epsilon_l}:=G_2.
\end{equation}

\begin{equation} \label{eq:bounded_gradient}
    ||\nabla \log (\langle \mathbf{x}_t, \mathbf{y}_t) \rangle||_\infty \leq \frac{\epsilon_u }{\epsilon_l}:=G_\infty.
\end{equation}

Since we will compare multiple algorithms we introduce the notation: $R_T(\mathcal A)$, when speaking about the regret at time $T$ of an online learner $\mathcal A$, the same notation applies with the total regret $R_T^C$ or the regret on the costs $C_T$, defined in Section \ref{ch:transaction_costs}.

\section{Algorithm with regret bound}

As already pointed out, most algorithms in the Online Portfolio Optimization literature do not consider transaction costs and have guarantees only on the standard regret $R_T$. In this section we will summarize the most relevant algorithms for the Online Portfolio Optimization problem that have been proven to have only bounded regret $R_T$. 

\subsection{Universal Portfolios}\label{sec:UP}
The Universal Portfolios (UP) \cite{cover1996universal} algorithm has the best theoretical guarantees among the algorithm for Online Portfolio Optimization. 

\begin{definition}(Universal Portfolios).
\begin{equation}\label{eq:UP}
\mathbf x_{t+1}=\frac{\int_{\Delta}\mathbf x W_t(\mathbf x)d\mathbf x}{\int_{\Delta} W_t(\mathbf x)d\mathbf x}.
\end{equation}
\end{definition}

Note that this algorithm is the Continuous Mixture Forecaster for exp-concave losses, described in Section \ref{sec:exp-concave-mixture}, since the logarithmic loss is exp concave with $\nu=1$. With the analysis described in Section \ref{sec:laplace_mixture}.

Hence we have that 
\begin{equation}
R_T(UP)\le(N-1)\log(T+1).
\end{equation}

Clearly, the UP algorithm is computationally hard as it involves integration over a $N$-simplex. Indeed, there is an extensive research that looks into efficient implementations of the UP algorithm \cite{kalai2002efficient}.

Equation \eqref{eq:UP} can be refined with:

\begin{equation}\label{eq:general_UP}
\mathbf x_{t+1}=\frac{\int_{\Delta}\mathbf x W_t(\mathbf x)\mu(\mathbf x)d\mathbf x}{\int_{\Delta} W_t(\mathbf x)\mu(\mathbf x)d\mathbf x},
\end{equation}

where $\mu(\mathbf x)$ is a distribution over $\Delta_{N-1}$, there are choices of $\mu(\mathbf x)$ for which we can obtain slightly better constants for the regret bound.


\subsection{Exponential Gradient}

The Exponential Gradient (EG) algorithm is a specification of the OMD algorithm described in Section \ref{sec:OMD}, by using as the Bregman divergence the Kullback–Leibler divergence $d_\psi(\mathbf x,\mathbf y)=KL(\mathbf x,\mathbf y)=\sum\limits_{i=1}^Nx_i\log(x_i/y_i)$, and $\eta_t=\eta$ as constant sequence of learning rates. The update rule for EG in this case becomes:

\begin{definition}(Exponential Gradient).
\begin{equation}\label{eq:update_EG}
\mathbf x_{t+1}=\arg\min\limits_{x\in\Delta_{N-1}} KL(\mathbf x,\mathbf x_t)-\eta_t\left\langle \frac{\mathbf x_t}{\langle \mathbf x_t,\mathbf y_t\rangle},\mathbf x-\mathbf x_t\right\rangle.
\end{equation}
\end{definition}

The update rule in Equation \eqref{eq:update_EG} can be solved analytically \cite{helmbold1998line}, giving the following closed update 

\begin{equation}\label{eq:update_EG_closed}
x_{i,t+1}=\frac{x_{j,t}\exp\left(\eta_t{y_{j,t}}/\langle\mathbf x_t,\mathbf y_t\rangle\right)}{\sum\limits_{j=1}^Nx_{j,t}\exp\left(\eta_t{y_{j,t}}/\langle\mathbf x_t,\mathbf y_t\rangle\right)}, \forall i\in1,\ldots,N.
\end{equation}

This update rule is also of the kind of an Weighted Average Forecaster described in Section \ref{sec:existence_of_no_regret}, in particular it is a special kind of Exponentially Weighted Forecaster of Definition \ref{def:ewf}.

We know that $\psi(\mathbf x)=\sum\limits_{i=1}^Nx_i\log(x_i)$ is $1$-strong convex with respect to the $L_1$ norm $||\cdot||_1$ \cite{shalev2007online}, and so we have that $KL(\mathbf x,\mathbf x_t)\ge\frac{1}{2}||\mathbf x-\mathbf x_t||_1$.

Moreover, we we can bound the $L_1$ diameter $D_1$ of the simplex $\Delta_{N-1}$ as: 
$$D_1=\sup\limits_{\mathbf x,\mathbf y\in\Delta_{N-1}}||\mathbf x-\mathbf y||_1\le2.$$

Therefore, we can apply straightforward Theorem \ref{th:regret_omd} with $\eta=\frac{1}{G_\infty}\sqrt{\frac{2\log N}{T}}$ and $\mathbf x_1=(1/N,\ldots,1/N)$ giving as a result the following regret bound:

\begin{equation}
R_T(EG)\le \frac{\epsilon_u}{\epsilon_l}\sqrt{\frac{T\log N}{2}}.
\end{equation}


\subsection{Online Newton Step}

The Online Newton Step (ONS) \cite{hazan2007logarithmic} algorithm is one of the few algorithm other than the UP algorithm that guarantees a logarithmic bound $R_T(ONS)=\mathcal O(\log T)$. The method uses second order information of the loss function, but it can nonetheless stated into first order method such as OMD. 

\begin{definition}(Online Newton Step).
\begin{equation}\label{eq:update_ONS}
\mathbf x_{t+1}=\Pi^{A_t}_{\Delta_{N-1}}\left(\mathbf x_t+\frac{1}{\beta}A_t^{-1}\frac{\mathbf y_t}{\langle\mathbf x_t,\mathbf y_t\rangle}\right),
\end{equation}
where $\prod^{A_t}_{\Delta_{N-1}}$ is the non-standard projection onto the simplex $\Delta_{N-1}$ defined as 
$$\Pi^{A_t}_{\Delta_{N-1}}(\mathbf x_0):=\arg\inf\limits_{\mathbf x\in\Delta_{N-1}}\langle\mathbf x-\mathbf x_0,A_t(\mathbf x-\mathbf x_0)\rangle.$$

and the matrix $A_t\in\mathbb R^{N\times N}$ it is defined as $A_t=\sum\limits_{s=1}^t \nabla f_t(\mathbf x_t)\nabla f_t(\mathbf x_t)^T+\epsilon\mathbb I_N$.
\end{definition}

The idea for the ONS algorithm is originated from the concept of strong convexity, that is defined as follow:

\begin{definition}(Strong Convexity).\label{def:strong_cnvx}
A function $f:\mathcal D\to\mathbb R$ is said to be $\mu$-strong convex w.r.t. the norm $||\cdot||$ if: 
$$f(y)-f(x)\ge\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}||y-x||^2,\forall x,y\in\mathcal D,\forall x,y\in\mathcal D.$$
\end{definition}

Usually there is the correspondence of convex-loss $R_T=\mathcal O(\sqrt T)$ and strong-convex loss $R_T=\mathcal O(\log T)$. The idea of the ONS algorithm is to recover a weaker concept of strong convexity for exp-concave losses:

\begin{definition}(Weak Strong Convexity).\label{def:weak_strong_cnvx}
A function $f:\mathcal D\to\mathbb R$ is said to be weak-strong convex if $\forall x\in\mathcal D\exists A$ such that: 
$$f(y)-f(x)\ge\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}||y-x||_{A}^2,$$
for a matrix $A$ that defines the norm $||x||^2_{A}=\langle x, Ax\rangle$.
\end{definition}

In fact for any $\nu$ exp-concave function $f:\mathcal D\to\mathbb R$ with bounded gradient, \emph{i.e.} $||\nabla f(\mathbf x)||_2\le G\ \forall \mathbf x\in\mathcal D$, with $D=\sup\limits_{\mathbf x,\mathbf y\in\mathcal X}||\mathbf x-\mathbf y||_2$, $\beta=\frac{1}{2}\min\{\nu,\frac{1}{4GD}\}$ and $A=\nabla f(\mathbf x)\nabla f(\mathbf x)^T$, we have that: 

\begin{equation}\label{eq:weak_strong_conv_exp_concave}
f(\mathbf y)-f(\mathbf x)\ge\langle\nabla f(x),\mathbf y-\mathbf x\rangle+\frac{\beta}{2}||\mathbf y-\mathbf x||_{A}\forall x,y\in\mathcal D.
\end{equation}

The main idea of ONS exploit the weak-strong convexity of exp-concave functions to recover $\mathcal O(\log T)$ regret bounds, the complete proof can be found in \cite{hazan2007logarithmic}.

From Equation \eqref{eq:weak_strong_conv_exp_concave} we can see that the matrix $A_t$ used by the ONS algorithm is just a lower bound on the Hessian of the loss function. This is also why the projection onto the simplex of the ONS algorithm is the non standard projection defined by the matrix $A_t$.

Finally, choosing $\beta=\frac{\alpha}{8\sqrt{N}}$ the regret bound for the ONS algorithm becomes:

\begin{equation}\label{eq:regret_ONS}
R_T(ONS)\le\frac{10 N^{3/2}}{\epsilon_l}\log\left(\frac{NT}{\epsilon_l^2}\right)
\end{equation}

\section{Algorithm with total regret bound}

To the best of our knowledge there are only two works that bound the total regret bound $R_T^C$ defined in Chapter \ref{ch:transaction_costs}. We will present the works and discuss their limitations, that we tried to solve with our approach.

\subsection{Online Lazy Updates}

Online Lazy Updates (OLU) \cite{das2013online} is an algorithm designed to minimize explicitly the total regret $R_T^C$. The origin of this algorithm has to be traced back to a generalization of the OMD algorithm discussed in Section \ref{sec:OMD}. Namely, the generalization of the OMD algorithm that we are referring to, is the Composite Objective Mirror Descent (COMID) algorithm \cite{duchi2010composite}. The idea behind the COMID algorithm is the have a composite loss function of the kind $g_t(\mathbf x)=f_t(\mathbf x) + r(\mathbf x)$, then the algorithm linearizes the first term $f_t(\mathbf x)$ of the composite loss (as in OMD), but does not linearize the second term $r(\mathbf x)$ of the composite loss $g_t(\mathbf x)$. Both terms of the loss function, $f_t$ and $r$, are assumed to be convex.

\begin{definition}(Composite Objective Mirror Descent).\label{def:COMID}
\begin{equation}\label{eq:update_COMID}
    \mathbf{x}_{t+1}=\arg \hspace{-0.1cm} \min\limits_{\mathbf{x} \in \Delta_{M-1}} \hspace{-0.1cm} \left\{ \eta \langle \nabla f_t(\mathbf{x}_t), \mathbf{x} \rangle + \eta \ r(\mathbf{x}) + d_\psi(\mathbf{x}, \mathbf{x}_t) \right\},
\end{equation}

Where $d_\psi$ is the Bregman divergence for a convex function $\psi$. 

\end{definition}

A lemma similar to Lemma \ref{th:OMD_first_th} gives the following guarantees to the regret of a learner using COMID:

\begin{lemma}(\cite{duchi2010composite} Theorem 2.2)
$\forall \mathbf x\in\Delta_{N-1}$ and for a sequence $\{\mathbf x_t\}_{t=1}^T$ defined by the update rule \eqref{eq:update_COMID}, we have:
\begin{equation}
\eta\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+r(\mathbf x_t)-r(\mathbf x)]\le d_\psi(\mathbf x,\mathbf x_t)+\eta r(\mathbf x_1)+\frac{\eta^2}{2\alpha}\sum\limits_{t=1}^T||\nabla f_t(\mathbf x_t)||_*^2,
\end{equation} 
where $\alpha$ is the parameter that ensures $d_\psi(\mathbf x,\mathbf y)\ge \frac{\alpha}{2}||\mathbf x-\mathbf y||^2$.
\end{lemma}

This lemma implies easily regret bound on the regret $R_T$. If we assume that the losses $f_t$ have bounded gradient by $G_*$ under the norm $||\cdot||_*$ then we have that: 

\begin{equation}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+r(\mathbf x_t)-r(\mathbf x)]\le\frac{1}{\eta}d_\psi(\mathbf x,\mathbf x_t)+r(\mathbf x_1)+\frac{T\eta}{2\alpha}G_*^2.
\end{equation}

Consequently, taking $\eta=\frac{K}{\sqrt T}$ we obtain, and assuming $d_\psi(\mathbf x,\mathbf y)\le D\ \forall\mathbf x,\mathbf y\in\Delta_{N-1}$, and assuming $r(\mathbf x_1)\le D_1$ we obtain:

\begin{equation}\label{eq:regret_comid_final}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+r(\mathbf x_t)-r(\mathbf x)]\le KD\sqrt{T} + D_1+\frac{\sqrt{T}}{2\alpha}G_*^2.
\end{equation}

The idea of OLU is to take $r=r_t(\mathbf x)=\gamma||\mathbf x-\mathbf x_{t-1}||_1$ \cite{das2014online}, $\psi=||\mathbf x||_2^2$ generating the following update rule:

\begin{definition}(Online Lazy Update).\label{def:update_OLU}
\begin{equation}\label{eq:update_COMID}
    \mathbf{x}_{t+1}=\arg \hspace{-0.1cm} \min\limits_{\mathbf{x} \in \Delta_{M-1}} \hspace{-0.1cm} \left\{ -\eta\log(\langle \mathbf x,\mathbf y_t\rangle) + \eta \gamma ||\mathbf x_t-\mathbf x||_1 + \frac{1}{2}||\mathbf x-\mathbf x_t||^2_2 \right\},
\end{equation}

\end{definition}

Note that there are multiple definitions of the OLU algorithm, and we reported a version in which the first term of the loss has note been linearized. Linearization of the first term of the loss with $\langle\nabla f_t(\mathbf x_t),\mathbf x\rangle$ would results in the same update rule and same analysis (since the loss $f_t$ is convex).

With this specifications we obtain the result from \eqref{eq:regret_comid_final}:

\begin{equation}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+\gamma||\mathbf x_t-\mathbf x_{t-1}||_1-\gamma||\mathbf x-\mathbf x_{t-1}||_1]\le \left( \frac{1}{K} + \frac{N K \epsilon_u^2 }{2 \epsilon_l^2} \right) \sqrt{T}.
\end{equation}

Then taking to the left hand side the terms $\gamma||\mathbf x-\mathbf x_{t-1}||_1$, and specializing $f_t(\mathbf x)$ as the log-loss defined for the Online Portfolio Optimization framework, we obtain:

\begin{equation}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+\gamma||\mathbf x_t-\mathbf x_{t-1}||_1]\le \sum\limits_{t=1}^T\gamma||\mathbf x-\mathbf x_{t-1}||_1+\left( \frac{1}{K} + \frac{N K \epsilon_u^2 }{2 \epsilon_l^2} \right) \sqrt{T}.
\end{equation}

Now the left hand side is equivalent to our Definition \ref{def:totoal_regret} of total regret $R_T^C$. Note that we do not have a sub-linear bound for the total regret yet. In order to recover the sub-linear bound on the total regret $R_T^C$ in \cite{das2014online} (Theorem 1) the authors assume $\gamma=\frac{K'}{\sqrt{T}}$. With this assumption we can recover the following bound on the total regret for the OLU algorithm:

\begin{equation}
R_T^C(OLU)\le2K'\sqrt{T}+\left( \frac{1}{K} + \frac{N K \epsilon_u^2 }{2 \epsilon_l^2} \right) \sqrt{T}.
\end{equation}

It is clear from our discussion on the model for Online Portfolio Optimization with transaction costs described in Chapter \ref{ch:transaction_costs}, that $\gamma>0$ is fixed and independent on the time horizon $T$ of the investment process. \todo{magari mettere quelsta discussione su OLU come th?}


\subsection{Universal Portfolios with Transaction Costs}

In \cite{blum1999universal} the authors extended the ideas of the UP algorithm \ref{sec:UP} to include transaction costs. The approach is heavily inspired by the results in Online Learning of Section \ref{sec:laplace_mixture}, namely, the Laplace Mixture Forecaster for the log-loss, and it differs substantially with our approach that is inspired by the Online Convex Optimization framework.

Indeed the Laplace mixture forecaster has the property \cite{cover1996universal} that the wealth of the Laplace Mixture Forecaster is the average wealth of the wealth of the expert class. The idea followed by the authors is to show that is the portfolios in the expert class are paying transaction costs then the regret experienced by the algorithm \todo{vorrei non mettere UCP perché non lo capisco}

\section{Other Related Works}

There are also heuristic algorithms designed to exploit some known phenomena in markets. Among these heuristic algorithm we can find, Anticor~\cite{borodin2004can}, PAMR~\cite{li2012pamr}, OLMAR~\cite{li2015moving}, and MRTC~\cite{yang2018reversion}, which in some cases outperform the algorithms described above in terms of empirical performance. 
Remarkably, none of the above algorithms provide guarantees on the regret, and so we will avoid an in depth description of their mechanism, since we are currently concerned with algorithm that provide theoretical guarantees without assumptions on the distribution of the marker vectors.

\todo{rephrase}