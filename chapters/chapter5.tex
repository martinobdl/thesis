\chapter{Algorithms for the Online Portfolio Optimization Problem}\label{ch:algos}

In this section we will review the state of the art algorithms for the Online Portfolio Optimization problem and discuss their theoretical guarantees, and how these algorithms can be generated by the theoretical framework of Online Learning with expert advice and Online Optimization we described in Chapter \ref{ch:OnlineLearning}.

The setting is the one described in Section \ref{sec:OPO}, in particular $\Delta=\Delta_{N-1}\subset \mathbb R^N$ is the $N$-simplex, and an element $\mathbf x_t\in\Delta$ describes the allocation over $N$ stocks for the $t$-th period.

As is commonly done in the portfolio allocation literature~\cite{agarwal2006algorithms}, we assume that the price of the assets does not change too much during two consecutive rounds, or, formally:

\begin{assumption} \label{ass:nojunk}
    %  Given the OPO framework, 
     There exist two finite constants $\epsilon_l, \epsilon_u \in \mathbb{R}^+$ s.t.~the price relatives~$y_{j,t} \in [\epsilon_l, \epsilon_u]$, with $0 < \epsilon_l \leq \epsilon_u < +\infty$, for each round $t \in \{ 1, \ldots, T \}$ and each asset $j \in \{1, \ldots, N \}$.
\end{assumption}

Notice that under Assumption~\ref{ass:nojunk} it is possible to bound the $L_1$, $L_2$ and the $L_\infty$ gradient of the loss as follows:\todo{check}

\begin{equation} \label{eq:bounded_gradient_1}
    ||\nabla \log (\langle \mathbf{x}_t, \mathbf{y}_t) \rangle||_1 \leq \frac{N\epsilon_u}{\epsilon_l}:=G_1,
\end{equation}

\begin{equation} \label{eq:bounded_gradient_2}
    ||\nabla \log (\langle \mathbf{x}_t, \mathbf{y}_t) \rangle||_2 \leq \frac{\epsilon_u \sqrt{N}}{\epsilon_l}:=G_2,
\end{equation}

\begin{equation} \label{eq:bounded_gradient_3}
    ||\nabla \log (\langle \mathbf{x}_t, \mathbf{y}_t) \rangle||_\infty \leq \frac{\epsilon_u }{\epsilon_l}:=G_\infty.
\end{equation}

Since we will compare multiple algorithms, we introduce the notation $R_T(\mathcal A)$ when speaking about the regret at time $T$ of an online learner $\mathcal A$. The same notation applies with the total regret $R_T^C$ or the regret on the costs $C_T$, defined in Section \ref{ch:transaction_costs}.

\section{Algorithm with regret bound}

As already pointed out, most algorithms in the Online Portfolio Optimization literature do not consider transaction costs and have guarantees only on the standard regret $R_T$. In this section we will summarize the most relevant algorithms for the Online Portfolio Optimization problem that have been proven to have only bounded regret $R_T$. 

\subsection{Universal Portfolios}\label{sec:UP}
The Universal Portfolios (UP) \cite{cover1996universal} algorithm has been one of the first algorithms that have been introduced in the framework of Online Portfolio Optimization. The UP algorithm has the best theoretical guarantees among the algorithm for Online Portfolio Optimization, as it can reach the minmax value of the game between the adversarial environment and the learning agent (Theorem 10.2 \cite{cesa2006prediction}).


\begin{definition}(Universal Portfolios).
The prediction of the UP algorithm is the following:
\begin{equation}\label{eq:UP}
\mathbf x_{t+1}=\frac{\int_{\Delta}\mathbf x W_t(\mathbf x)d\mathbf x}{\int_{\Delta} W_t(\mathbf x)d\mathbf x}.
\end{equation}
\end{definition}

Note that this algorithm is the Continuous Mixture Forecaster for exp-concave losses, described in Section \ref{sec:exp-concave-mixture}, since the logarithmic loss is exp-concave with $\nu=1$. as described in the analysis of Section \ref{sec:laplace_mixture}.

Hence, we have that: 
\begin{equation}
R_T(UP)\le(N-1)\log(T+1).
\end{equation}

Clearly the UP algorithm is computationally hard as it involves integration over a $N$-simplex. Indeed, there is an extensive research that looks into efficient implementations of the UP algorithm \cite{kalai2002efficient}.


Moreover, the update rule in Equation \eqref{eq:UP} can be generalized as follow:

\begin{equation}\label{eq:general_UP}
\mathbf x_{t+1}=\frac{\int_{\Delta}\mathbf x W_t(\mathbf x)\mu(\mathbf x)d\mathbf x}{\int_{\Delta} W_t(\mathbf x)\mu(\mathbf x)d\mathbf x},
\end{equation}

where $\mu(\mathbf x)$ is a distribution over $\Delta_{N-1}$, the standard UP algorithm is obtained by choosing $\mu$ as the uniform distribution over the probability simplex, but there are choices of $\mu(\mathbf x)$ for which we can obtain slightly better constants for the regret bound.

% \subsection{Implementation of Universal Portfolios}

% To implement Universal Portfolio


\subsection{Exponential Gradient}

The Exponential Gradient (EG) algorithm is a specification of the OMD algorithm described in Section \ref{sec:OMD}, by using the Kullback–Leibler divergence $d_\psi(\mathbf x,\mathbf y)=KL(\mathbf x,\mathbf y)=\sum\limits_{i=1}^Nx_i\log(x_i/y_i)$ as the Bregman divergence the, and $\eta_t=\eta$ as the constant sequence of learning rates. The update rule for EG in this case becomes:

\begin{definition}(Exponential Gradient). The EG algorithm is defined by the following update rule:
\begin{equation}\label{eq:update_EG}
\mathbf x_{t+1}=\arginf\limits_{\mathbf x\in\Delta_{N-1}} \left\{KL(\mathbf x,\mathbf x_t)-\eta_t\left\langle \frac{\mathbf x_t}{\langle \mathbf x_t,\mathbf y_t\rangle},\mathbf x-\mathbf x_t\right\rangle\right\}.
\end{equation}
\end{definition}

The update rule in Equation \eqref{eq:update_EG} can be solved analytically \cite{helmbold1998line}, giving the following closed update 

\begin{equation}\label{eq:update_EG_closed}
x_{i,t+1}=\frac{x_{j,t}\exp\left(\eta_t{y_{j,t}}/\langle\mathbf x_t,\mathbf y_t\rangle\right)}{\sum\limits_{j=1}^Nx_{j,t}\exp\left(\eta_t{y_{j,t}}/\langle\mathbf x_t,\mathbf y_t\rangle\right)}, \forall i\in1,\ldots,N.
\end{equation}

This update rule is also a Weighted Average Forecaster described in Section \ref{sec:existence_of_no_regret}, and in particular, it is a special case of Exponentially Weighted Forecaster of Definition \ref{def:ewf}. This is useful for proving the following theorem.

\begin{theorem}(Regret Bound for the Exponential Gradient Algorithm).
The EG algorithm defined by the update rule in Equation \eqref{eq:update_EG_closed} has the following regret bound:
\begin{equation}
R_T(EG)\le \frac{\epsilon_u}{\epsilon_l}\sqrt{\frac{T\log N}{2}}.
\end{equation}
\end{theorem}

\begin{proof}
We know that $\psi(\mathbf x)=\sum\limits_{i=1}^Nx_i\log(x_i)$ is $1$-strong convex with respect to the $L_1$ norm $||\cdot||_1$ \cite{shalev2007online}, and so we have that $KL(\mathbf x,\mathbf x_t)\ge\frac{1}{2}||\mathbf x-\mathbf x_t||_1$.

Moreover, we can bound the $L_1$ diameter $D_1$ of the simplex $\Delta_{N-1}$ as: 
$$D_1=\sup\limits_{\mathbf x,\mathbf y\in\Delta_{N-1}}||\mathbf x-\mathbf y||_1\le2.$$

Therefore, we can apply Theorem \ref{th:regret_omd} with $\eta=\frac{1}{G_\infty}\sqrt{\frac{2\log N}{T}}$ and $\mathbf x_1=(1/N,\ldots,1/N)$ giving as a result the thesis.
\end{proof}

Note that one could also obtain a regret bound by using the fact that the EG algorithm is a specialization of OMD with $\psi(\mathbf x)=\sum\limits_{i=1}^N x_i\log(x_i)$ 


\subsection{Online Newton Step}

The Online Newton Step (ONS) \cite{hazan2007logarithmic} algorithm is one of the few algorithms other than the UP one, that guarantees a logarithmic bound $R_T(ONS)=\mathcal O(\log T)$. The method uses second order information of the loss function, but it can nonetheless be stated into first order method such as OMD, as we will discuss at the end of this section.

\begin{definition}(Online Newton Step).
The ONS algorithm is defined by the following update rule:
\begin{equation}\label{eq:update_ONS}
\mathbf x_{t+1}=\Pi^{A_t}_{\Delta_{N-1}}\left(\mathbf x_t+\frac{1}{\beta}A_t^{-1}\frac{\mathbf y_t}{\langle\mathbf x_t,\mathbf y_t\rangle}\right),
\end{equation}
where $\prod^{A_t}_{\Delta_{N-1}}(\cdot)$ is the non-standard projection onto the simplex $\Delta_{N-1}$ defined as 
\begin{equation}\label{eq:non_standard_proj}
\Pi^{A_t}_{\Delta_{N-1}}(\mathbf x_0):=\arginf\limits_{\mathbf x\in\Delta_{N-1}}\langle\mathbf x-\mathbf x_0,A_t(\mathbf x-\mathbf x_0)\rangle,
\end{equation}

and the matrix $A_t\in\mathbb R^{N\times N}$ is defined as 
\begin{equation}\label{eq:matrix_ONS}
A_t=\sum\limits_{s=1}^t \nabla f_t(\mathbf x_t)\nabla f_t(\mathbf x_t)^T+\epsilon\mathbb I_N,
\end{equation},
where $\mathbb I_N$ si the identity matrix in $\mathbb R^N$.
\end{definition}

The idea for the ONS algorithm is originated from the concept of strong convexity, that is defined as follow:

\begin{definition}(Strong Convexity).\label{def:strong_cnvx}
A function $f:\mathcal D\to\mathbb R$ is said to be $\mu$-strong convex w.r.t. the norm $||\cdot||$ if: 
$$f(y)-f(x)\ge\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}||y-x||^2,\forall x,y\in\mathcal D,\forall x,y\in\mathcal D.$$
\end{definition}

Usually there is the correspondence of convex-loss $R_T=\mathcal O(\sqrt T)$ and strong-convex loss $R_T=\mathcal O(\log T)$. The idea of the ONS algorithm is to recover a weaker concept of strong convexity for exp-concave losses:

\begin{definition}(Weak Strong Convexity).\label{def:weak_strong_cnvx}
A function $f:\mathcal D\subset \mathbb R^N\to\mathbb R$ is said to be weak-strong convex if $\forall x\in\mathcal D\exists A\in\mathbb R^{N\times N}$ such that: 
$$f(y)-f(x)\ge\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}||y-x||_{A}^2,$$
for a positive defined matrix $A$ that defines the norm $||x||^2_{A}=\langle x, Ax\rangle$.
\end{definition}

Indeed, for any $\nu$ exp-concave function $f:\mathcal D\to\mathbb R$ with bounded gradient, \emph{i.e.} $||\nabla f(\mathbf x)||_2\le G\ \forall \mathbf x\in\mathcal D$, with $D=\sup\limits_{\mathbf x,\mathbf y\in\mathcal X}||\mathbf x-\mathbf y||_2$, $\beta=\frac{1}{2}\min\{\nu,\frac{1}{4GD}\}$ and $A=\nabla f(\mathbf x)\nabla f(\mathbf x)^T$, we have that: 

\begin{equation}\label{eq:weak_strong_conv_exp_concave}
f(\mathbf y)-f(\mathbf x)\ge\langle\nabla f(x),\mathbf y-\mathbf x\rangle+\frac{\beta}{2}||\mathbf y-\mathbf x||_{A}\forall x,y\in\mathcal D.
\end{equation}

The main idea of ONS is exploiting the weak-strong convexity of exp-concave functions to recover $\mathcal O(\log T)$ regret bounds. The complete proof can be found in \cite{hazan2007logarithmic}.

From Equation \eqref{eq:weak_strong_conv_exp_concave} we can see that the matrix $A$ used by the ONS algorithm is just a lower bound on the Hessian of the loss function. This is also the reason why the projection onto the simplex of the ONS algorithm is the non standard projection defined by the matrix $A_t$ defined in Equation \eqref{eq:matrix_ONS}.

\begin{theorem}(Regret Bound for the Online Newtonw Step Algorithm).

By choosing $\beta=\frac{\alpha}{8\sqrt{N}}$ in Equation \eqref{eq:update_ONS}, the regret bound for the ONS algorithm becomes:

\begin{equation}\label{eq:regret_ONS}
R_T(ONS)\le\frac{10 N^{3/2}}{\epsilon_l}\log\left(\frac{NT}{\epsilon_l^2}\right)
\end{equation}
\end{theorem}

ONS can also be seen as a specification of OMD \cite{luo2018efficient} by choosing an adaptive regularizer $\psi(\mathbf x)=\psi_t(\mathbf x)=\frac{1}{2}||\mathbf x||_{A_t}^2$ where $A_t$ is defined as $A_t=A_{t-1}+\nabla f_t(\mathbf x_t)\nabla f_t(\mathbf x_t)^T$, for a positive defined $A_0$. In this case the gradient of the Fenchel Conjugate becomes $\nabla \psi_t^*(\mathbf x)=\Pi^{A_t}_{\Delta_{N-1}}(\mathbf x)$, defined in Equation \eqref{eq:non_standard_proj}.

\section{Algorithm with total regret bound}

To the best of our knowledge there are only two works that bound the total regret $R_T^C$ defined in Chapter \ref{ch:transaction_costs}. We will present the works and discuss their limitations, that we tried to solve with our approach.

\subsection{Online Lazy Updates}

Online Lazy Updates (OLU) \cite{das2013online} is an algorithm designed to minimize explicitly the total regret $R_T^C$. The origin of this algorithm has to be traced back to a generalization of the OMD algorithm discussed in Section \ref{sec:OMD}. Namely, the generalization of the OMD algorithm that we are referring to is the Composite Objective Mirror Descent (COMID) algorithm \cite{duchi2010composite}. The idea behind the COMID algorithm is to have a composite loss function of the kind $g_t(\mathbf x)=f_t(\mathbf x) + r(\mathbf x)$, then the algorithm linearizes the first term $f_t(\mathbf x)$ of the composite loss (as in OMD) but does not linearize the second term $r(\mathbf x)$ of the composite loss $g_t(\mathbf x)$. Both terms of the loss function, $f_t$ and $r$, are assumed to be convex.

\begin{definition}(Composite Objective Mirror Descent).\label{def:COMID}
The COMID algorithm is defined with the following update equation:
\begin{equation}\label{eq:update_COMID}
    \mathbf{x}_{t+1}=\arginf\limits_{\mathbf{x} \in \Delta_{M-1}} \hspace{-0.1cm} \left\{ \eta \langle \nabla f_t(\mathbf{x}_t), \mathbf{x} \rangle + \eta \ r(\mathbf{x}) + d_\psi(\mathbf{x}, \mathbf{x}_t) \right\},
\end{equation}

where $d_\psi$ is the Bregman divergence for a convex function $\psi$. 

\end{definition}

A lemma similar to Lemma \ref{th:OMD_first_th} gives the following guarantees to the regret of a learner using COMID:

\begin{lemma}(\cite{duchi2010composite} Theorem 2.2)
$\forall \mathbf x\in\Delta_{N-1}$ and for a sequence $\{\mathbf x_t\}_{t=1}^T$ defined by the update rule \eqref{eq:update_COMID}, we have:
\begin{equation}
\eta\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+r(\mathbf x_t)-r(\mathbf x)]\le d_\psi(\mathbf x,\mathbf x_t)+\eta r(\mathbf x_1)+\frac{\eta^2}{2\alpha}\sum\limits_{t=1}^T||\nabla f_t(\mathbf x_t)||_*^2,
\end{equation} 
where $\alpha$ is the parameter that ensures $d_\psi(\mathbf x,\mathbf y)\ge \frac{\alpha}{2}||\mathbf x-\mathbf y||^2$.
\end{lemma}

This lemma implies a regret bound on $R_T$. If we assume that the losses $f_t$ have bounded gradient by $G_*$ under the norm $||\cdot||_*$ then we have that: 

\begin{equation}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+r(\mathbf x_t)-r(\mathbf x)]\le\frac{1}{\eta}d_\psi(\mathbf x,\mathbf x_t)+r(\mathbf x_1)+\frac{T\eta}{2\alpha}G_*^2.
\end{equation}

Consequently, taking $\eta=\frac{K}{\sqrt T}$, and assuming $d_\psi(\mathbf x,\mathbf y)\le D\ \forall\mathbf x,\mathbf y\in\Delta_{N-1}$, and $r(\mathbf x_1)\le D_1$, we obtain:

\begin{equation}\label{eq:regret_comid_final}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+r(\mathbf x_t)-r(\mathbf x)]\le KD\sqrt{T} + D_1+\frac{\sqrt{T}}{2\alpha}G_*^2.
\end{equation}

The idea of OLU is to take $r=r_t(\mathbf x)=\gamma||\mathbf x-\mathbf x_{t-1}||_1$ \cite{das2014online}, $\psi=||\mathbf x||_2^2$ generating the following update rule:

\begin{definition}(Online Lazy Update).\label{def:update_OLU}
The OLU algorithm is defined by the following update rule:
\begin{equation}\label{eq:update_OLU}
    \mathbf{x}_{t+1}=\arginf\limits_{\mathbf{x} \in \Delta_{M-1}} \hspace{-0.1cm} \left\{ -\eta\log(\langle \mathbf x,\mathbf y_t\rangle) + \eta \gamma ||\mathbf x_t-\mathbf x||_1 + \frac{1}{2}||\mathbf x-\mathbf x_t||^2_2 \right\}.
\end{equation}

\end{definition}

Note that there are multiple definitions of the OLU algorithm, and we reported a version in which the first term of the loss has not been linearized. Linearization of the first term of the loss with $\langle\nabla f_t(\mathbf x_t),\mathbf x\rangle$ would result in the same update rule and same analysis (since the loss $f_t$ is convex).

With this specifications we obtain the result from Equation \eqref{eq:regret_comid_final}:

\begin{equation}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+\gamma||\mathbf x_t-\mathbf x_{t-1}||_1-\gamma||\mathbf x-\mathbf x_{t-1}||_1]\le \left( \frac{1}{K} + \frac{N K \epsilon_u^2 }{2 \epsilon_l^2} \right) \sqrt{T}.
\end{equation}

Then, taking to the left hand side the terms $\gamma||\mathbf x-\mathbf x_{t-1}||_1$, and specializing $f_t(\mathbf x)$ as the log-loss defined for the Online Portfolio Optimization framework, we obtain:

\begin{equation}
\sum\limits_{t=1}^T[f_t(\mathbf x_t)-f_t(\mathbf x)+\gamma||\mathbf x_t-\mathbf x_{t-1}||_1]\le \sum\limits_{t=1}^T\gamma||\mathbf x-\mathbf x_{t-1}||_1+\left( \frac{1}{K} + \frac{N K \epsilon_u^2 }{2 \epsilon_l^2} \right) \sqrt{T}.
\end{equation}

Now the left hand side is equivalent to our Definition \ref{def:totoal_regret} of total regret $R_T^C$. Note that we do not have a sub-linear bound for the total regret yet. In order to recover the sub-linear bound on the total regret $R_T^C$ in \cite{das2014online} (Theorem 1) the authors assume $\gamma=\frac{\gamma_0}{\sqrt{T}}$. With this assumption we can recover the following bound on the total regret for the OLU algorithm:

\begin{equation}
R_T^C(OLU)\le2\gamma_0\sqrt{T}+\left( \frac{1}{K} + \frac{N K \epsilon_u^2 }{2 \epsilon_l^2} \right) \sqrt{T}.
\end{equation}

It is clear from our discussion on the model for Online Portfolio Optimization with transaction costs described in Chapter \ref{ch:transaction_costs} that $\gamma>0$ is fixed and independent on the time horizon $T$ of the investment process. \todo{F: Scriviamola prima questa cosa di $\gamma$ costante}
\todo{add ADMM}

\subsection{Implementation of Online Lazy Update}

Due to the non-smooth $L_1$ term in Equation \eqref{eq:update_OLU}, we need a special optimization procedure. The authors proposed the Alternating Direction Method of Multipliers (ADMM) scheme \cite{boyd2011distributed}, by decoupling the non-smooth term $||\mathbf x_t-\mathbf x||_1$ from the rest of the objective function.
Indeed, Equation \eqref{eq:update_OLU} is equivalent to 
\begin{equation}
\mathbf x_{t+1}=\argmin\limits_{\mathbf x\in\Delta,\mathbf x-\mathbf x_t=\mathbf z}\left\{-\eta\log(\langle\mathbf x,\mathbf y_t\rangle)+\eta\gamma||\mathbf z||_1+\frac{1}{2}||\mathbf x-\mathbf x_t||_2^2\right\}
\end{equation}

The ADMM method is concerned with optimization problems of the kind

\begin{equation}\label{eq:general_ADMM}
\begin{cases}
\inf f(\mathbf x)+g(\mathbf z) \\
s.t.\ A\mathbf x+B\mathbf z=\mathbf c,
\end{cases}
\end{equation}
where $\mathbf x,\mathbf z,\mathbf c\in\mathbb R^N$ and $A,B\in\mathbb R^N$.

Problem \eqref{eq:general_ADMM} has augmented Lagrangian:

\begin{equation}\label{eq:lagrangian_1}
\mathcal L_\rho(\mathbf x,\mathbf z,\mathbf y )=f(\mathbf x)+g(\mathbf x) + \langle\mathbf y,A\mathbf x+B\mathbf z-\mathbf c\rangle +\frac{\rho^2}{2}||A\mathbf x+B\mathbf z-\mathbf c||_2^2.
\end{equation}

Now ADMM solves the Lagrangian problem by iterating over minimization on the primal variables and then doing a dual update (this justifies the name \emph{alternating direction} in ADMM) with the following update rules:

\begin{equation}\label{eq:update_ADMM_1}
\begin{cases}
(\mathbf x^{k+1},\mathbf z^{k+1})=\arginf\limits_{\mathbf x,\mathbf z }\mathcal L_\rho(\mathbf x,\mathbf z,\mathbf y^{(k)})\\
\mathbf y^{(k+1)}=\mathbf y^{(k)}+\rho(A\mathbf x^{k+1}+B\mathbf z^{(k+1)}-\mathbf c)
\end{cases}
\end{equation}

If we define the residual $\mathbf r=A\mathbf x+B\mathbf z-\mathbf c$ and $\mathbf u=\frac{1}{\rho}\mathbf y$ as the scaled dual variable, then the Lagrangian in Equation \eqref{eq:lagrangian_1} turns into 

\begin{equation}\label{eq:lagrangian_2}
\mathcal L_\rho(\mathbf x,\mathbf z,\mathbf u )=f(\mathbf x)+g(\mathbf x) + \frac{\rho}{2}||\mathbf r+\mathbf u||_2^2-\frac{\rho}{2}||\mathbf u||_2^2
\end{equation}
So we can rewrite the update Equations \eqref{eq:update_ADMM_1} as reported in Algorithm \ref{alg:ADMM}.

\begin{algorithm}[!h]
    \caption{Alternating Direction Method of Multipliers}
    \label{alg:ADMM}
    \begin{algorithmic}[1]
    \REQUIRE $f,g,A,B,\mathbf c,\mathbf x^0,\mathbf z^0,\mathbf u^0,\rho$ \nonumber
    \WHILE{Stopping condition not met:}
    \STATE Update the primal variables: 
    $$(\mathbf x^{k+1},\mathbf z^{k+1})=\arginf\limits_{\mathbf x,\mathbf z }\left\{f(\mathbf x)+g(\mathbf z)+\frac{\rho}{2}||A\mathbf x^{(k)}+B\mathbf z^{(k)}-\mathbf c+\mathbf u^{(k)}||\right\}$$
    \STATE Update the dual variable: 
    $$\mathbf u^{(k+1)}=\mathbf u^{(k)}+A\mathbf x^{k+1}+B\mathbf z^{(k+1)}-\mathbf c$$
    \ENDWHILE
    \RETURN $\mathbf x^k,\mathbf z^k$. 
    \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:ADMM} is know to converge (see \cite{boyd2011distributed} Appendix A).

In order to use ADMM to solve the optimization of OLU in Equation \eqref{eq:update_OLU}, we have do the following identifications at each time $t$:

\begin{equation}
\begin{cases}
f(\mathbf x)&=-\eta\nabla f_t(\mathbf x_t)\\
g(\mathbf z)&=\gamma\eta||\mathbf z||_1 \\
\mathbf z&=\mathbf x-\mathbf x_t\\
A&=\mathbb I_N\\
B&=-\mathbb I_N\\
\mathbf c&=\mathbf x_t
\end{cases},
\end{equation}
and then use Algorithm \ref{alg:ADMM} to solve Equation \eqref{eq:update_OLU}.



% \subsection{Universal Portfolios with Transaction Costs}

% In \cite{blum1999universal} the authors extended the ideas of the UP algorithm \ref{sec:UP} to include transaction costs. The approach is heavily inspired by the results in Online Learning of Section \ref{sec:laplace_mixture}, namely, the Laplace Mixture Forecaster for the log-loss, and it differs substantially with our approach that is inspired by the Online Convex Optimization framework.

% Indeed the Laplace mixture forecaster has the property \cite{cover1996universal} that the wealth of the Laplace Mixture Forecaster is the average wealth of the wealth of the expert class. The idea followed by the authors is to show that is the portfolios in the expert class are paying transaction costs then the regret experienced by the algorithm \todo{vorrei non mettere UCP perché non lo capisco}

\section{Other Related Works}

There are also heuristic algorithms designed to exploit some known phenomena in markets. Among these heuristic algorithm we can find Anticor~\cite{borodin2004can}, PAMR~\cite{li2012pamr}, OLMAR~\cite{li2015moving}, and MRTC~\cite{yang2018reversion}, which in some cases outperform the algorithms described above in terms of empirical performance. 
Remarkably, none of the above algorithms provide guarantees on the regret, and so we will avoid an in-depth description of their mechanism, since we are currently concerned with algorithms that provide theoretical guarantees without assumptions on the distribution of the marker vectors.